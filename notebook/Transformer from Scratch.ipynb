{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc912525-3cce-4489-ad31-74f530a8eff1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dea73c-a7ee-44d7-9c66-57a25addebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966d985-fb98-4ed6-83da-0fa4e616f6ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ff3e7-8fff-4765-bfcc-465277a710ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## Scaled Dot Product Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c078706-709c-448d-9591-e977fd8ddb49",
   "metadata": {},
   "source": [
    "$\n",
    "\\LARGE\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\\LARGE\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553514b4-d475-4dfc-a825-06ce3bcdef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    q, k, v, mask=0):\n",
    "    scores = q @ k.transpose(-1, -2)      # q @ k.T\n",
    "    scaled = scores / (q.shape[-1]**.5)   # * 1 / sqrt(dim)\n",
    "    scaled = scaled + mask                # + mask\n",
    "    attention = F.softmax(scaled, dim=-1) # softmax()\n",
    "    value = attention @ v                 # @ v\n",
    "    return value, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd75c8f-4849-458c-b250-f47776e52b06",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f42a44-73cc-48c7-a419-4f71a65efceb",
   "metadata": {},
   "source": [
    "#### Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e611240-7a59-41d4-96f8-f7960015b1f8",
   "metadata": {},
   "source": [
    "$\n",
    "M = \\begin{bmatrix}\n",
    "0 & -\\infty & -\\infty & -\\infty \\\\\n",
    "0 & 0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & 0 & -\\infty \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f803e9c-dd47-4fe4-b1e6-5753fc8504ef",
   "metadata": {},
   "source": [
    "#### Padding Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab6497-6f59-4bc1-91da-5e7bf2faddb0",
   "metadata": {},
   "source": [
    "$\n",
    "M = \\begin{bmatrix}\n",
    "0 & 0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & -\\infty & -\\infty \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d52b1-f350-491c-abbb-14c3c1d40d14",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8982a0a-cf8d-46e4-a977-efee11caa2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_mask = lambda l=4, v=(0,float('-inf')) : torch.tensor([[v[0] if b <= a else v[1] for b in range(l)] for a in range(l)])\n",
    "padding_mask = lambda l=4, i=2, v=(0, float('-inf')) : torch.tensor([v[0]] * i + [v[1]] * (l - i))[:l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deaf2f6e-bece-41c3-9e23-d3dcedafb054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " value      torch.Size([2, 10, 3])\n",
      " attention  torch.Size([2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "shape = [2, 10, 3] # B S D\n",
    "l = nn.Linear(shape[-1], shape[-1]*3)\n",
    "x = torch.randn(shape)\n",
    "Wqkv = l(x)\n",
    "q, k, v = torch.chunk(Wqkv, chunks=3, dim=-1)\n",
    "value, attention = scaled_dot_product_attention(q, k, v, mask=0)\n",
    "print(f\" value      {value.shape}\\n attention  {attention.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa81bf0-0d9d-4801-9645-1b206a04c702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "383205bf-bc34-4770-8207-e3029544b0b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2849fa-d901-434f-a9e6-cef5e2f5741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(s, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, f\"cant split {embed_dim} embed_dim to {num_heads} heads\"\n",
    "        \n",
    "        # :int\n",
    "        s.edim = embed_dim\n",
    "        s.head = num_heads\n",
    "        s.hdim = embed_dim // num_heads\n",
    "        \n",
    "        # :nn.Linear\n",
    "        s.Wq = nn.Linear(embed_dim, embed_dim)\n",
    "        s.Wk = nn.Linear(embed_dim, embed_dim)\n",
    "        s.Wv = nn.Linear(embed_dim, embed_dim)\n",
    "        s.Wo = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(s, xq, xk, xv, mask=0):\n",
    "        \n",
    "        # Linear\n",
    "        q = s.Wq(xq) # B, S, ED\n",
    "        k = s.Wk(xk)\n",
    "        v = s.Wv(xv)\n",
    "\n",
    "        hq = s.split_heads(q) # B, H, S, HD\n",
    "        hk = s.split_heads(k)\n",
    "        hv = s.split_heads(v)\n",
    "\n",
    "        value, attention = scaled_dot_product_attention(hq, hk, hv, mask)\n",
    "        \n",
    "        combined = s.combine_heads(value) # B, S, ED\n",
    "        output = s.Wo(combined)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "    def split_heads(s, x):                               # B, S, ED\n",
    "        xh = x.reshape(x.shape[:-1] + (s.head, s.hdim))  # B, S, (H, HD)\n",
    "        return xh.transpose(-2, -3)                      # B, (H, S), HD\n",
    "\n",
    "    def combine_heads(s, x):                             # B, H, S, HD\n",
    "        hs = x.transpose(-2, -3)                         # B, (S, H), HD\n",
    "        return hs.reshape(hs.shape[:-2] + (s.edim,))     # B, S, (ED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245ff64b-93cf-478e-9219-10be9c3cbb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "\n",
    "m = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "y, a = m(x, x, x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ba5f6-cb4d-44c4-9cb7-6ae92a50c283",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7b2dc-c20a-41a2-af7c-8704d16a2b62",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "\\LARGE\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n",
    "$\n",
    "\n",
    "$\n",
    "\\LARGE\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2\n",
    "$\n",
    "\n",
    "$\n",
    "\\LARGE\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\LARGE y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bf1fe2-86e5-4e42-bc8f-6cb4c4505200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(s, shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        s.reshape(shape)\n",
    "        s.eps = eps\n",
    "\n",
    "    def reshape(s, shape):\n",
    "        s.shape = (shape,) if type(shape) is int else shape\n",
    "        s.gamma = nn.Parameter(torch.ones(*s.shape))\n",
    "        s.beta = nn.Parameter(torch.zeros(*s.shape))\n",
    "\n",
    "    def forward(s, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        std = torch.sqrt(var + s.eps)\n",
    "        x_norm = (x - mean) / std\n",
    "        y = x_norm * s.gamma + s.beta\n",
    "\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fd89a-32bb-4889-b4d6-b592da238179",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Position Wise Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329e6dd5-9488-4bfe-93ab-fc0f268f2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(s, embed_dim, d_ff=0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if not d_ff: d_ff = embed_dim * 4\n",
    "        s.linear1 = nn.Linear(embed_dim, d_ff)\n",
    "        s.linear2 = nn.Linear(d_ff, embed_dim)\n",
    "        s.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(s, x):\n",
    "        x = s.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = s.dropout(x)\n",
    "        x = s.linear2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c600f01a-71bc-464f-ab6b-812755c97b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 10\n",
    "d_ff = 20\n",
    "m = PositionWiseFeedForward(embed_dim, d_ff)\n",
    "\n",
    "x = torch.randn(2, 2, embed_dim)\n",
    "y = m(x)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240c256-50a5-46a7-ab57-f1b585d5a05e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bb930e0-f375-4079-97d9-1be92c5cc3d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(s, embed_dim, seq_len=5000):\n",
    "        super().__init__()\n",
    "        s.pe = positional_encoding(seq_len, embed_dim)\n",
    "\n",
    "    def forward(s, x):\n",
    "        x = x + s.pe[:x.size(-2),:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9474c-454d-4480-9d76-1fc1ea4a6c3b",
   "metadata": {},
   "source": [
    "$\n",
    "\\LARGE\\text PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right) \\text\n",
    "$\n",
    "\n",
    "$\n",
    "\\LARGE\\text PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right) \\text\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7602563f-426e-41cc-9d9a-61a9f735316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positional_encoding = lambda s, d, n=10000 : torch.Tensor([[ ( np.sin(k/np.power(n, i/d)) if i % 2 == 0 else np.cos(k/np.power(n, (i-1)/d)) ) for i in range(d)] for k in range(s)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f829e27c-e0d3-4fd9-a5d5-0951349096fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4790,  1.2248, -0.9709],\n",
       "         [ 0.6315,  0.3719, -0.1971]],\n",
       "\n",
       "        [[ 0.6122,  1.2353, -0.7712],\n",
       "         [ 1.9904, -0.8597, -1.3386]],\n",
       "\n",
       "        [[-1.7526,  2.1024, -0.1130],\n",
       "         [ 1.6807,  1.3510, -1.5821]],\n",
       "\n",
       "        [[ 1.2562,  0.9885, -1.7097],\n",
       "         [ 1.3562,  1.2714, -0.1139]],\n",
       "\n",
       "        [[-0.2333,  0.8334, -2.4092],\n",
       "         [ 1.1264,  0.7290,  0.6172]],\n",
       "\n",
       "        [[ 0.5313,  1.3085, -0.6649],\n",
       "         [ 0.4347,  0.8596,  1.0100]],\n",
       "\n",
       "        [[ 2.3336,  0.2377,  0.7649],\n",
       "         [ 1.4001, -0.6894, -1.6972]],\n",
       "\n",
       "        [[-0.8829,  1.6883,  0.0192],\n",
       "         [ 3.7390,  0.3130, -0.1684]],\n",
       "\n",
       "        [[-0.7948,  1.4359,  0.5092],\n",
       "         [ 1.8898,  1.3564,  0.6197]],\n",
       "\n",
       "        [[-0.0524,  3.7284,  0.0861],\n",
       "         [ 0.0645,  1.9532,  0.2532]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 2, 3)\n",
    "pe = positional_encoding(*x.shape[-2:])\n",
    "x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c861c7b0-8ce2-45af-8169-a15e63e4d31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 4\n",
    "seq_len = 2\n",
    "pe = PositionalEncoding(embed_dim)\n",
    "x = torch.zeros(5, seq_len, embed_dim)\n",
    "y = pe(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fc179-8b68-48f5-8533-b8d65a0c1c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7fa793d-8019-47bd-b71f-274b51db4a1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920e333-f279-4437-9ea0-275774c6b839",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d1631d0-92da-4f4c-bd6b-94aeb75c46d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(s, embed_dim, num_heads, d_ff=0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if not d_ff: d_ff = embed_dim * 4\n",
    "        \n",
    "        s.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        s.mha_dropout = nn.Dropout(dropout)\n",
    "        s.mha_norm = LayerNorm(embed_dim)\n",
    "        \n",
    "        s.ffn = PositionWiseFeedForward(embed_dim, d_ff)\n",
    "        s.ffn_dropout = nn.Dropout(dropout)\n",
    "        s.ffn_norm = LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(s, x, _=0, mask=0):\n",
    "        \n",
    "        v, _ = s.mha(x, x, x, mask=mask)\n",
    "        a    = s.mha_dropout(v) + x\n",
    "        x    = s.mha_norm(a)\n",
    "\n",
    "        f    = s.ffn(x)\n",
    "        a    = s.ffn_dropout(f) + x\n",
    "        x    = s.ffn_norm(a)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8908acc7-8c33-46d7-9a62-7de3dcde558a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "\n",
    "e = EncoderLayer(embed_dim, num_heads)\n",
    "\n",
    "x = torch.randn(2, 10, embed_dim)\n",
    "y = e(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47e561-206b-49f8-8d46-fa5ca74d82db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b737f9a9-326b-4915-9c01-7ebbb07e5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(s, embed_dim, num_heads, d_ff=0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if not d_ff: d_ff = embed_dim * 4\n",
    "\n",
    "        s.m_mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        s.m_mha_dropout = nn.Dropout(dropout)\n",
    "        s.m_mha_norm = LayerNorm(embed_dim)\n",
    "        \n",
    "        s.c_mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        s.c_mha_dropout = nn.Dropout(dropout)\n",
    "        s.c_mha_norm = LayerNorm(embed_dim)\n",
    "\n",
    "        s.p_ffn = PositionWiseFeedForward(embed_dim, d_ff)\n",
    "        s.p_ffn_dropout = nn.Dropout(dropout)\n",
    "        s.p_ffn_norm = LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(s, x, ey=None, mask=0):\n",
    "        \n",
    "        if ey is None: ey = x\n",
    "\n",
    "        v, _ = s.m_mha(x, x, x, mask=mask)\n",
    "        a    = s.m_mha_dropout(v) + x\n",
    "        x    = s.m_mha_norm(a)\n",
    "        \n",
    "        v, _ = s.c_mha(x, ey, ey, mask=mask)\n",
    "        a    = s.c_mha_dropout(v) + x\n",
    "        x    = s.c_mha_norm(a)\n",
    "\n",
    "        f    = s.p_ffn(x)\n",
    "        a    = s.p_ffn_dropout(f) + x\n",
    "        x    = s.p_ffn_norm(a)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a96e73e-16cd-497b-8ff7-74c176352cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "\n",
    "d = DecoderLayer(embed_dim, num_heads)\n",
    "\n",
    "x = torch.randn(2, 10, embed_dim)\n",
    "y = d(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5c0c9-12bd-4d25-b8ef-7d0461d457bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer Layer Customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe45ddc-325c-442b-b6cc-45980697ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = lambda *args : TransformerLayer(*args, layer=TransformerLayer.encoder_layer)\n",
    "decoder_layer = lambda *args : TransformerLayer(*args, layer=TransformerLayer.decoder_layer)\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    \n",
    "    def __init__(s, embed_dim, num_heads, d_ff=0, dropout=0.1, layer=decoder_layer):\n",
    "        super().__init__()\n",
    "        s.layer = s.getLayer(layer=layer, embed_dim=embed_dim, num_heads=num_heads, d_ff=d_ff, dropout=dropout)\n",
    "    \n",
    "    encoder_layer = ['mha', 'ff']          # Encoder - MultiHeadAttention + PositionWiseFeedForward\n",
    "    decoder_layer = ['mha', 'cmha', 'ff']  # Decoder - MultiHeadAttention + Cross-MultiHeadAttention + PositionWiseFeedForward\n",
    "    \n",
    "    blocks_init = {\n",
    "        'mha'  : lambda *args, embed_dim, num_heads,      **kwargs : MultiHeadAttention(embed_dim, num_heads),\n",
    "        'cmha' : lambda *args, embed_dim, num_heads,      **kwargs : MultiHeadAttention(embed_dim, num_heads),\n",
    "        'ff'   : lambda *args, embed_dim, d_ff, dropout,  **kwargs : PositionWiseFeedForward(embed_dim, d_ff, dropout),\n",
    "        \n",
    "        'do'   : lambda *args, dropout,                   **kwargs : nn.Dropout(dropout),\n",
    "        'ln'   : lambda *args, embed_dim,                 **kwargs : LayerNorm(embed_dim),\n",
    "    }\n",
    "    \n",
    "    blocks_forward = {\n",
    "        'mha'  : lambda *args, f, x, ey, mask, **kwargs : f(x, x, x, mask=mask)[0],\n",
    "        'cmha' : lambda *args, f, x, ey, mask, **kwargs : f(x, ey, ey, mask=mask)[0],\n",
    "        'ff'   : lambda *args, f, x, ey, mask, **kwargs : f(x),\n",
    "    }\n",
    "    \n",
    "    def getLayer(s, layer, **kwargs):\n",
    "        init = TransformerLayer.blocks_init\n",
    "        forward = TransformerLayer.blocks_forward\n",
    "        return [\n",
    "            {\n",
    "                'name': b,\n",
    "                'layer': init[b](**kwargs),\n",
    "                'forward': forward[b],\n",
    "                'postprocess': {\n",
    "                    'dropout': init['do'](**kwargs),\n",
    "                    'norm': init['ln'](**kwargs),\n",
    "                }\n",
    "            }\n",
    "        for b in layer]\n",
    "\n",
    "    def blockForward(s, **kwargs):\n",
    "        x = 0\n",
    "        for b in s.layer:\n",
    "            v = b['forward'](f=b['layer'], **kwargs)\n",
    "            a = b['postprocess']['dropout'](v) + x\n",
    "            x = b['postprocess']['norm'](a)\n",
    "        return x\n",
    "    \n",
    "    def forward(s, x, ey=None, mask=0):\n",
    "        if ey is None: ey = x\n",
    "        return s.blockForward(x=x, ey=ey, mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3187a91-4da1-4904-8153-06d92206558d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'mha',\n",
       "  'layer': MultiHeadAttention(\n",
       "    (Wq): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wk): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wv): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wo): Linear(in_features=9, out_features=9, bias=True)\n",
       "  ),\n",
       "  'forward': <function __main__.TransformerLayer.<lambda>(*args, f, x, ey, mask, **kwargs)>,\n",
       "  'postprocess': {'dropout': Dropout(p=0.1, inplace=False),\n",
       "   'norm': LayerNorm()}},\n",
       " {'name': 'cmha',\n",
       "  'layer': MultiHeadAttention(\n",
       "    (Wq): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wk): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wv): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wo): Linear(in_features=9, out_features=9, bias=True)\n",
       "  ),\n",
       "  'forward': <function __main__.TransformerLayer.<lambda>(*args, f, x, ey, mask, **kwargs)>,\n",
       "  'postprocess': {'dropout': Dropout(p=0.1, inplace=False),\n",
       "   'norm': LayerNorm()}},\n",
       " {'name': 'ff',\n",
       "  'layer': PositionWiseFeedForward(\n",
       "    (linear1): Linear(in_features=9, out_features=36, bias=True)\n",
       "    (linear2): Linear(in_features=36, out_features=9, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  ),\n",
       "  'forward': <function __main__.TransformerLayer.<lambda>(*args, f, x, ey, mask, **kwargs)>,\n",
       "  'postprocess': {'dropout': Dropout(p=0.1, inplace=False),\n",
       "   'norm': LayerNorm()}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "\n",
    "d = decoder_layer(embed_dim, num_heads)\n",
    "\n",
    "x = torch.randn(2, 10, embed_dim)\n",
    "y = d(x)\n",
    "d.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8543ca40-65bf-465e-b170-81943faba97f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'mha',\n",
       "  'layer': MultiHeadAttention(\n",
       "    (Wq): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wk): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wv): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wo): Linear(in_features=9, out_features=9, bias=True)\n",
       "  ),\n",
       "  'forward': <function __main__.TransformerLayer.<lambda>(*args, f, x, ey, mask, **kwargs)>,\n",
       "  'postprocess': {'dropout': Dropout(p=0.1, inplace=False),\n",
       "   'norm': LayerNorm()}},\n",
       " {'name': 'cmha',\n",
       "  'layer': MultiHeadAttention(\n",
       "    (Wq): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wk): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wv): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (Wo): Linear(in_features=9, out_features=9, bias=True)\n",
       "  ),\n",
       "  'forward': <function __main__.TransformerLayer.<lambda>(*args, f, x, ey, mask, **kwargs)>,\n",
       "  'postprocess': {'dropout': Dropout(p=0.1, inplace=False),\n",
       "   'norm': LayerNorm()}},\n",
       " {'name': 'ff',\n",
       "  'layer': PositionWiseFeedForward(\n",
       "    (linear1): Linear(in_features=9, out_features=36, bias=True)\n",
       "    (linear2): Linear(in_features=36, out_features=9, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  ),\n",
       "  'forward': <function __main__.TransformerLayer.<lambda>(*args, f, x, ey, mask, **kwargs)>,\n",
       "  'postprocess': {'dropout': Dropout(p=0.1, inplace=False),\n",
       "   'norm': LayerNorm()}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl = decoder_layer(9, 3)\n",
    "tl.layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557575a-9570-49e5-b049-d86429f2916a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9189a1c-4092-45fd-80f7-c9287a397e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayers(nn.Module):\n",
    "\n",
    "    def __init__(s, \n",
    "                 vocab_size, seq_len, \n",
    "                 embed_dim, num_heads, \n",
    "                 d_ff=0, dropout=0.1, \n",
    "                 layer=encoder_layer, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        s.embeding = nn.Embedding(vocab_size, embed_dim)\n",
    "        s.pe = PositionalEncoding(embed_dim, seq_len)\n",
    "        \n",
    "        s.layers = nn.ModuleList([\n",
    "            layer(embed_dim, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(s, x, ey=0, mask=0):\n",
    "        \n",
    "        x = s.embeding(x)\n",
    "        x = s.pe(x)\n",
    "\n",
    "        for layer in s.layers:\n",
    "            x = layer(x, ey, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d9fc839-202c-4b68-8387-4868c544986c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "seq_len = 2\n",
    "embed_dim = 9\n",
    "num_heads = 3\n",
    "\n",
    "encoder = TransformerLayers(\n",
    "    vocab_size, seq_len, embed_dim, num_heads, \n",
    "    layer=EncoderLayer, num_layers=2\n",
    ")\n",
    "\n",
    "x = torch.randint(0, vocab_size, (3, 2))\n",
    "\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cfcb6-b918-4b17-93b1-99f6ac123a1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f527b17-9f4a-40e2-991a-2b6aebf2cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(s, \n",
    "                 vocab_size, src_len, tgt_len, \n",
    "                 embed_dim, num_heads, \n",
    "                 d_ff=0, dropout=0.1, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        s.encoder = TransformerLayers(\n",
    "            vocab_size, src_len, embed_dim, num_heads,\n",
    "            d_ff=d_ff, dropout=dropout,\n",
    "            layer=encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        s.decoder = TransformerLayers(\n",
    "            vocab_size, tgt_len, embed_dim, num_heads,\n",
    "            d_ff=d_ff, dropout=dropout,\n",
    "            layer=decoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        s.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(s, src=None, tgt=None, src_mask=0, tgt_mask=0):\n",
    "        x = None\n",
    "        if src is not None: x = s.encoder(src, mask=src_mask)\n",
    "        if tgt is not None: x = s.decoder(tgt, x, mask=tgt_mask + get_mask(tgt.size(-1)))\n",
    "\n",
    "        if tgt is not None:\n",
    "            x = s.linear(x)\n",
    "            # x = torch.softmax(x, dim=-1) # CrossEntropyLoss Loss\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def generate(s, num_tokens=10, tgt=None, tgt_mask=0):\n",
    "        lt = lambda a : torch.stack(a)[..., :, -1] # tokens, batch, pred\n",
    "        o = [tgt]\n",
    "        for i in range(num_tokens):\n",
    "            f = s(tgt=o[-1], tgt_mask=tgt_mask)\n",
    "            pred = torch.argmax(f, dim=-1)\n",
    "            o.append(pred)\n",
    "        return lt(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fb21128-1197-4c5b-91fb-0d825c182d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 2\n",
    "seq_len = 4\n",
    "embed_dim = 3\n",
    "num_heads = 1\n",
    "\n",
    "t = Transformer(vocab_size, seq_len, seq_len, embed_dim, num_heads, num_layers=2)\n",
    "\n",
    "xg = lambda b=10 : torch.randint(0, vocab_size, (b, seq_len,))\n",
    "yg = lambda b=10 : torch.randint(0, vocab_size, (b, seq_len,))\n",
    "\n",
    "# print( t(src=x, tgt=y).shape )\n",
    "# print( t(src=x).shape )\n",
    "# print( t(tgt=y).shape )\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(t.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# optimizer = optim.Adam(t.parameters(), lr=0.000005)\n",
    "optimizer = optim.Adam(t.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "t.train()\n",
    "for i in range(1):\n",
    "    y = yg()\n",
    "    optimizer.zero_grad()\n",
    "    dy = t(tgt=y)\n",
    "    \n",
    "    loss = criterion(dy.view(-1, vocab_size), y.view(-1))\n",
    "    # loss = criterion(dy, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred = torch.argmax(dy, dim=-1)\n",
    "\n",
    "o = t.generate(tgt=yg(1))\n",
    "o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec832738-f15a-4e62-b31a-97ebfb06dc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4ec1b0-8358-4e5e-af23-10d78b0476e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73436722-a24d-4c60-9d4b-c22d24a8c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "auto_map = lambda fn, arr, *arg: [fn(e, *arg) if type(e) is not list else auto_map(fn, e, *arg) for e in arr]\n",
    "\n",
    "# [1,2,3] -> {(1,2):1, (2,3):1}\n",
    "def count_pair(token, state={}):\n",
    "    for pair in zip(token, token[1:]): # (0, 1), (1, 2), (2, 3)\n",
    "        state[pair] = state.get(pair, 0) + 1 # pair:count\n",
    "    return state\n",
    "\n",
    "# [1,2,3], (1,2), 257 -> [257,3]\n",
    "def merge(token, pair, replace):\n",
    "    y = []; i = 0; lt = len(token)\n",
    "    while i < lt:\n",
    "        if token[i] == pair[0] and i+1 < lt and token[i+1] == pair[1]:\n",
    "            y.append(replace); i += 2\n",
    "        else:\n",
    "            y.append(token[i]); i += 1\n",
    "    return y\n",
    "\n",
    "# [1,[2,[3,4]]] -> [1,2,3,4]\n",
    "flattern = lambda arr : a if (a := [i for a in arr for i in (a if type(a) is list else [a])]) and all([type(i) is not list for i in a]) else flattern(a)\n",
    "\n",
    "# [[[1,2]][[[3, 4]]] -> [[1,2],[3,4]]\n",
    "keep_last_dim = lambda v : keep_last_dim([i for arr in v for i in arr]) if type(v[0][0]) is list else v # merges first dim\n",
    "\n",
    "\n",
    "class BytePairEncoding:\n",
    "    \n",
    "    def __init__(s):\n",
    "        pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        s.regex = regex.compile(pattern)\n",
    "        s.char_size = 256 # UTF-8 | 0-255\n",
    "        s.vocab = {i:i for i in range(s.char_size)}; # 258: [1, 7, 9] | idx:values\n",
    "        s.merges = {}; # (1, 7): 257 | pair:new_id\n",
    "    \n",
    "    def split(s, text): return s.regex.findall(text) # ['Hello', ' world']\n",
    "    \n",
    "    def chr(s, idx): return chr(idx) # 65 -> 'A'\n",
    "    \n",
    "    def ord(s, text): return [ord(c) if ord(c) < s.char_size else s.char_size-1 for c in text] # 'a1A' -> [97, 49, 65]\n",
    "\n",
    "    def tokenize_text(s, text): return auto_map(s.ord, s.split(text)) # [[0,6], [1,7,6], [5,8,4]]\n",
    "    \n",
    "    def str(s, ids): return ''.join([s.chr(idx) for idx in ids]) # [97, 98, 99] -> 'abc\"\n",
    "    \n",
    "    def train(s, text, num_merges=3):\n",
    "        cs = s.char_size\n",
    "        tokens = s.tokenize(text) # [[0,6], [1,7,6], [5,8,4]]\n",
    "        \n",
    "        for i in range(cs, cs+num_merges):\n",
    "            state = {} # count pairs eg. = (1,2):3, (2,3):1\n",
    "            for token in tokens: count_pair(token, state) # update state\n",
    "            pair = max(state, key=state.get, default=0) # top pair | (1,2):3 | when 3 is max\n",
    "            if pair == 0: continue\n",
    "            tokens = [merge(token, pair, i) for token in tokens] # merge\n",
    "\n",
    "            s.merges[pair] = i\n",
    "            s.vocab[i] = flattern((s.vocab[pair[0]], s.vocab[pair[1]])) # i: [c1, c2, c3...] | flattern(vocab[idx]..)\n",
    "    \n",
    "    def encode_text(s, text):\n",
    "        tokens = s.tokenize(text) # [[0,6], [1,7,6], [5,8,4]]\n",
    "        for i in range(len(tokens)):\n",
    "        \twhile len(tokens[i]) > 1:\n",
    "        \t\tpairs = (set(zip(tokens[i], tokens[i][1:]))) # (256, 257), (257, 258)\n",
    "        \t\tpair = min([p for p in pairs if p in s.merges.keys()] or [0], key=lambda p: s.merges.get(p, 0)) # get earliest pair | (1, 6): 256 | when 256 is min\n",
    "        \t\tif pair == 0: break\n",
    "        \t\ttokens[i] = merge(tokens[i], pair, s.merges[pair])\n",
    "        return tuple(tokens)\n",
    "\n",
    "    def decode_tokens(s, tokens):\n",
    "        return s.str(flattern([s.vocab[idx] for idx in flattern(tokens)]))\n",
    "    \n",
    "    # wrapper functions\n",
    "    \n",
    "    def tokenize(s, o):\n",
    "        fn = s.tokenize_text\n",
    "        if type(o) is list: return keep_last_dim(auto_map(fn, o))\n",
    "        else: return fn(o)\n",
    "    \n",
    "    def encode(s, o):\n",
    "        fn = s.encode_text\n",
    "        if type(o) is list: return auto_map(fn, o)\n",
    "        else: return fn(o)\n",
    "    \n",
    "    def decode(s, o):\n",
    "        fn = s.decode_tokens\n",
    "        if type(o) is list: return auto_map(fn, o)\n",
    "        else: return fn(o)\n",
    "\n",
    "    # debug\n",
    "\n",
    "    def tokens(s):\n",
    "        added_vocabs = list(s.vocab.values())[s.char_size:]\n",
    "        return [s.decode((vocab,)) for vocab in added_vocabs]\n",
    "    \n",
    "    def test(s, text=\"test\"):\n",
    "        d = s.decode(s.encode(text))\n",
    "        print(\"Tokenizer Decode Pass\" if d == text else \"Tokenizer Decode Fail!\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365c951a-15c6-4c0d-8719-f84f0863354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello hello hello world world'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Hello hello hello world world\"\n",
    "t = BytePairEncoding()\n",
    "t.train(a, 4)\n",
    "\n",
    "e = (flattern(t.encode(a)),)\n",
    "d = t.decode(e)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8595c5c5-c2b9-4f4f-8810-431d2c8d9cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Decode Pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"tetetete tetetet\"]\n",
    "t = BytePairEncoding()\n",
    "t.train(text, 4)\n",
    "\n",
    "e = t.encode(text)\n",
    "t.decode(e)\n",
    "t.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153460a2-361f-4661-9fb6-c3fd55e488e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7b4ba33-1a32-4f39-8b3d-4ee39c55f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files :\n",
      "[136542963336478720] [part 10].txt\n",
      "\n",
      "chat examples :\n",
      "kiirby: What concert?! @coffee\n",
      "iwinalot7: Shadiverisity fucking sucks\n",
      "bearnadette: @greyasashe meirl\n",
      "iwinalot7: This is 2018 get a better mic\n",
      "bearnadette: except the car thing\n",
      "greyasashe: i know we all have your preconceptions about medieval snake people but what would they REALLY use\n",
      "iwinalot7: He's also cuz incorrect a lot of the time\n",
      "coffee: Kii what do you mean what concert\n",
      "bearnadette: also known as: which human race is most similar to snakes based on my perceptions of foreigners,\\nalso also known as: let's write a fantasy novel.\n",
      "iwinalot7: Idk I just hate that guy lmao\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"../dataset/dataset/discord/\"\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "chats = ''\n",
    "\n",
    "for file in files:\n",
    "\twith open(path+file, 'r', encoding=\"UTF-8\") as f:\n",
    "\t\tchats += f.read()\n",
    "\n",
    "\n",
    "chats = chats.split('\t')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\tfrom random import randint as r\n",
    "\trd = r(0, len(chats) - 10)\n",
    "    \n",
    "\tprint('files :')\n",
    "\t[print(i) for i in files]\n",
    "\tprint()\n",
    "\tprint('chat examples :')\n",
    "\t[print(chats[rd+i]) for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4bf69-a388-4593-abea-59c5cef1bde5",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab1d9c-1741-4343-b93e-c63df883910d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "701a9e8c-6de2-4a1e-b5bb-2427f44cb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import os\n",
    "\n",
    "default = {\n",
    "    'directory'   : '../dataset/',\n",
    "    'name'        : 'default',\n",
    "    'extension'   : '.pickle'\n",
    "}\n",
    "\n",
    "path = lambda name, extension = default['extension']  : default['directory'] + name + extension\n",
    "\n",
    "\n",
    "def load(name = default['name']):\n",
    "    with open(path(name), 'rb') as file: obj = pickle.load(file)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save(dat, name = default['name']):\n",
    "    os.makedirs(os.path.dirname(path(name)), exist_ok=True)\n",
    "    with open(path(name), 'wb') as file: pickle.dump(dat, file)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f0e2b-ca61-4c6d-bb6f-c9620f885dda",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968b287-ac7d-453d-98e7-28ac65632530",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee3b6ff7-5278-4ae3-b31e-24ec922f954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ~30 mins\\nmerges = 10000\\nlimit = merges*1//4\\ntokenizer = BytePairEncoding()\\ntokenizer.train(chats[:limit], merges)\\nsave(tokenizer, 'tokenizer')\\nprint(tokenizer.tokens())\\n#\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer Train \n",
    "\"\"\" ~30 mins\n",
    "merges = 10000\n",
    "limit = merges*1//4\n",
    "tokenizer = BytePairEncoding()\n",
    "tokenizer.train(chats[:limit], merges)\n",
    "save(tokenizer, 'tokenizer')\n",
    "print(tokenizer.tokens())\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af92e5e8-0e98-4d6d-8df5-4540324f1a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Decode Pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello world! how are you!', 'Hello world! how are you!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = load('tokenizer')\n",
    "a = [\"Hello world! how are you!\"]*batch_size\n",
    "e = [(flattern(i),) for i in t.encode(a)]\n",
    "d = t.decode(e)\n",
    "t.test(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d05d54-6112-45da-aac4-01c3890fe59b",
   "metadata": {},
   "source": [
    "### Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd2fab95-ba11-4eb4-a521-f55dbd94c909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) Loss : 9.170742988586426\n",
      "*  discord just told me to look be hind you while i loaded in.\n",
      "-   arenallall aren lood aren aren Mand\\nPatriots} Mand whis\\nPatriotshie loodpping\\nPatriots Mand Mand lood\n",
      "\n",
      "*  Like kabouter plop\\nIk word daar zoe move vaan\n",
      "-   Cured Wut Hes serious serious Wutppingpping:Cpping belive:C:C happen:Cpping stuffing\\nPatriots happen text\n",
      "\n",
      "*  @Moose teleports behind you nothing personell kid\n",
      "-   wheels\\nPatriots\\ndurr tele\\nPatriots:heart\\nPatriots ques\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriotsfortunately\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\n",
      "\n",
      "*  scream\n",
      "-   sat Lookthol Look emailshie Look\\non\\nonils tow\\non\\nIT\\nIT sat tow\\nonthol Dpping\n",
      "\n",
      "*  Tfw neither my brother or I quite remember how to get to our great aunts house\n",
      "-   DY COR tele embarrasingly lood lood lood ac teleppingpping tele\\nPatriots spe quespping\\nPatriots silverhie\\nPatriots\n",
      "\n",
      "*  Just wing it\n",
      "-   fracture nah:C jo jophph:C Murfre shore fils fils jo fils NOT fils jo\\nThey jo nah\n",
      "\n",
      "*  lels\n",
      "-  pping far Look COR farhieject\\non jo far\\nIT Look D\\non far tow\\nIT Look D D\n",
      "\n",
      "*  I'm sure it will be fine\n",
      "-   superbi sk\\nPatriots Cofauram sk\\nPatriots\\nPatriots\\nPatriots Mand loodough\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots lood\\nPatriots\n",
      "\n",
      "*  We are basically\n",
      "-   app:heart Look masc seriousilsilsils:C\\nIT Lmaooo tow embarrasingly Look Lookpping app Mand shore stitches\n",
      "\n",
      "*  I juzr cleaned the fuck outta my front door\n",
      "-   superbi plat tele\\nPatriots tele lood\\nPatriots lood lood lood\\nPatriots\\nPatriots lood\\nPatriots lood\\nPatriots\\nPatriots lood\\nPatriots lood\n",
      "\n",
      "*  anyone ever play the contra driving game?\n",
      "-  :heart Wut fils nah app Val itsppingppingpping\\nPatriots happenppingpping\\nPatriotsppingpping\\nPatriots\\nPatriotspping\n",
      "\n",
      "*  Fuck yeah Addie\\nClean that door\n",
      "-   ticklemeelmos\\nPatriots masc ac:C:C Look Look Lookpping towilsils shore Look Look:C:Cpping:C\n",
      "\n",
      "*  Pre clean gross door\n",
      "-  \\nPatriots\\nPatriots plat\\nPatriots\\nPatriots\\nPatriots\\nPatriots apparently\\nPatriots\\nPatriots\\nPatriots Motiv stuffing\\nPatriots\\nPatriots\\nPatriots\\nPatriots Motiv\\nPatriots tele\n",
      "\n",
      "*  The grumps played contra\n",
      "-  nconfused app\\nPatriots\\nPatriots fils\\nPatriots\\nPatriots\\nPatriotspping tele lood acc lood lood lood lood\\nPatriots\\nPatriots stuffing Look\n",
      "\n",
      "*  Post clean. Nice door\n",
      "-  \\nPatriots\\nPatriots\\nPatriots apparently fils\\nPatriots fils stuffingpping CORhie stuffing\\nPatriots\\nPatriots tele\\nPatriots achie\\nPatriots\\nPatriots\n",
      "\n",
      "*  Make that door your binch\n",
      "-   epilate Lmaooofortunately fils lood emailsils Sensual\\nPatriotspping:C lood drunkski lood loodpping\\nPatriots:C lood lood\n",
      "\n",
      "(1) Loss : 9.106074333190918\n",
      "(2) Loss : 9.13200855255127\n",
      "(3) Loss : 9.12682056427002\n",
      "(4) Loss : 9.177270889282227\n",
      "(5) Loss : 9.204119682312012\n",
      "(6) Loss : 9.219099998474121\n",
      "(7) Loss : 9.14529800415039\n",
      "(8) Loss : 9.10731315612793\n",
      "(9) Loss : 9.168336868286133\n",
      "(10) Loss : 9.076852798461914\n",
      "(11) Loss : 9.165790557861328\n",
      "(12) Loss : 9.149581909179688\n",
      "(13) Loss : 9.05872917175293\n",
      "(14) Loss : 9.13878345489502\n",
      "(15) Loss : 9.123917579650879\n",
      "(16) Loss : 9.070937156677246\n",
      "*  how did I not know this\n",
      "-   live\\nPatriots\\nPatriots Sonpping\\nPatriots stuffing7 tele\\nPatriots serious COR lood text ac lood ac\\nPatriots\\nPatriots lood\n",
      "\n",
      "*  There's peanut butter capn crunch\n",
      "-  ? heardauram heardall\\nPatriots\\nPatriots Look essenti stuffing lood\\nPatriots\\nPatriots:heart\\nPatriots\\nPatriotspping mark\\nPatriots heard\n",
      "\n",
      "*  VC, we're you the one with the cereal tier list\\nI can't remember\n",
      "-  IS lood aren there lood MESSI Mand MESSI} lood:C MESSI lood MESSI Mand\\nPatriots fils:C\\nPatriots\\nPatriots\n",
      "\n",
      "*  Yes\n",
      "-   stuffing Look Look fartholthol jo Look\\nonppingtholthol farnTr shore\\nIT far far stuffingthol\n",
      "\n",
      "*  Sunsets are so romantic\n",
      "-   MORE War War\\nPatriots Wut belive:hearthie:heart masc Lmaooo\\nPatriots:heart app PM\\nPatriots serious NUCLEAR\\nPatriots stuffing\n",
      "\n",
      "*  who is that man\n",
      "-   movie Lmaooo Val maj Lmaooo Lmaooo Lmaooo straigh Lmaooo Lmaooo Lmaooo serious Cof Lmaooo7\\nPatriotshie\\nPatriots Lmaooo\\nPatriots\n",
      "\n",
      "*  It's George\n",
      "-   Mil\\nPatriots Lmaooo\\nPatriotshie fils fils PM dec fucking ticklemeelmos emails filsject essenti stuffingpping fils\\nPatriots\\nPatriots\n",
      "\n",
      "*  Costanza\n",
      "-  hiehietholthol nephews\\nIT sathie stuffing satilstholthol far stuffingthol stuffingtholthol\\nIT\n",
      "\n",
      "*  George Costanza\n",
      "-   serious essenti Lmaooo launch air Lmaooo7,7 Hes emails7 dec airpping apphie77 overly\n",
      "\n",
      "*  george of the jungle\n",
      "-   p serious happen fils\\nPatriots fils\\nPatriotspping:C ques\\nPatriots happen\\nPatriots\\nPatriots stuffing\\nPatriots\\nPatriots\\nPatriots\\nPatriots fils\n",
      "\n",
      "*  but wghy\n",
      "-   lood run launch\\nPatriots\\nPatriots launch\\nPatriots PM shore\\nPatriots\\nPatriots\\nPatriots\\nPatriots emails7\\nPatriots\\nPatriots\\nPatriots\\nPatriots jo\n",
      "\n",
      "*  This was my haul once\n",
      "-   emails emails decoration regular tow fils7:C fils:C majppingppingppingpping nah\\nPatriots filspping fils\n",
      "\n",
      "*  You don't know the legend of George Costanza?\n",
      "-  77 app nah nahppingpping emailsall:C happen Warpping silver:C\\nPatriots p}\\nPatriots\\nTop\n",
      "\n",
      "*  For research\n",
      "-  \\nit Lmaooo Lmaooo stuffing\\nPatriotsthol Lmaooohiehie Lmaooo Lmaooo\\nPatriots stuffing Look Look¤ stuffing\\nit teleports Lmaooo\n",
      "\n",
      "*  He is only the most important man\n",
      "-  auramgauram convinc Home ques\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots\\nPatriots lood\\nPatriots\\nPatriots\\nPatriots\\nPatriots\n",
      "\n",
      "*  George likes his chicken spicy\n",
      "-   serious aren belive p aren aren aren Look essenti far belive aren aren aren aren aren aren aren aren lood\n",
      "\n",
      "(17) Loss : 9.076737403869629\n",
      "(18) Loss : 9.060030937194824\n",
      "(19) Loss : 9.213244438171387\n",
      "(20) Loss : 9.128692626953125\n",
      "(21) Loss : 9.124910354614258\n",
      "(22) Loss : 9.081609725952148\n",
      "(23) Loss : 9.124625205993652\n",
      "(24) Loss : 9.132308959960938\n",
      "(25) Loss : 9.053549766540527\n",
      "(26) Loss : 9.092201232910156\n",
      "(27) Loss : 9.041777610778809\n",
      "(28) Loss : 9.100454330444336\n",
      "(29) Loss : 9.071264266967773\n",
      "(30) Loss : 9.176528930664062\n",
      "(31) Loss : 9.005537033081055\n",
      "(32) Loss : 9.03194522857666\n",
      "*  Moose is confused.\\nreally... really confused.\n",
      "-   Lmaooo app serious hunted Gr serious War\\nPatriots teleports\\nPatriots\\nPatriots\\nPatriots\\nPatriots War\\nPatriots\\nPatriots\\nPatriots there p\\nPatriots\n",
      "\n",
      "*  it's for humans\n",
      "-   Same ac its maj maj\\nPatriots masc7 masc fils7pping7\\nPatriots\\nPatriots\\nPatriots stuffing\\nPatriots maj app\n",
      "\n",
      "*  oh my PFP\n",
      "-   botteezVe jo jo nah joils dimin convincpping convinc jo\\nThey jopping:C jo jo jo\\nThey\n",
      "\n",
      "*  like... day 3 of a field ex.\\nconfused.\n",
      "-   PMSyzz ac ac lood PM review ac7 masc PM, thumb throw PM\\nPatriots masc masc, lesbian\n",
      "\n",
      "*  ya if you weren't in VC last night you wouldn't get it\\nlmao\n",
      "-   ac emails fils\\nPatriots serious\\nPatriots there:heart filsall serious its\\nPatriots app app Wutpping stuffing app ques\n",
      "\n",
      "*  lmaao you thought your actual face\n",
      "-  ÿ its\\nRadroaches serious aren lood lood aren lood lood aren lood aren aren lood lood lood aren lood aren\n",
      "\n",
      "*  no. Moose will never VC again.\n",
      "-   Gr app Lmaooo apparently\\nPatriots MESSI:heart, MESSI Sensual lood MESSI lood Mand Mand lood lood lood silver win\n",
      "\n",
      "*  @Moose see this\n",
      "-   Illinois need\\nRadroaches essenti spe essenti aren PM teleports\\nPatriots apparently aren PM aren\\nPatriots aren\\nPatriots aren7 spe\n",
      "\n",
      "*  too sexy a voice for vc?\n",
      "-  7 Hes ac ac belive serious7|7|, stuffing aren masc maj Look\\nPatriots p\\ndurr COR\n",
      "\n",
      "*  mmm... should i click while at the moosecake factory?\n",
      "-  \\nThey loodall emails Wut tow lood:C tele lood:C:C lovely:C:C\\nPatriots loodpping:C,\n",
      "\n",
      "*  interestinggg\n",
      "-   Lmaooo far overly emails\\nIT essentihie\\non ticklemeelmos Look NUCLEAR\\non\\nonish\\nonthol nephewsnTr silver Look\n",
      "\n",
      "*  pg 22 and 23\n",
      "-   app Wut fucking ac fight fils Lmaooo7 ac Lmaooo happen grabb Wut\\nPatriots\\nPatriots, fils fils fils ques\n",
      "\n",
      "*  No. I said like 3 words and crio was all holy heck you're canadian\n",
      "-   straigh apparently ac stuffing serious overly ac serious filsppingpping itspping fils serious quesppingpping apppping\n",
      "\n",
      "*  @MisterThief117 yea ill make sure to keep my eye out for her\n",
      "-   wheelsfortunately\\nPatriots Wut\\nPatriots\\nPatriots\\nPatriots\\nPatriots happen ques\\nPatriots lood lood silverpping silver_smiling\\nPatriots\\nTop\\nPatriots\n",
      "\n",
      "*  i still see underpants.\\nladies underpants.\n",
      "-  nTHIS Si Yess Yess Hes letting binge\\nPatriots tele\\nPatriots\\nPatriots\\nPatriots lood\\nPatriots\\nPatriots lood lood letting lood Kleenex\n",
      "\n",
      "*  I see london, I see france, I can see your underpants\n",
      "-   superbi decides MOREoll| aren Y office ac I ADA\\nPatriots aren I essenti lood\\nPatriots\\nPatriots\\nPatriots essenti\n",
      "\n",
      "(33) Loss : 9.022839546203613\n",
      "(34) Loss : 9.0765962600708\n",
      "(35) Loss : 9.142228126525879\n",
      "(36) Loss : 9.053421974182129\n",
      "(37) Loss : 9.050931930541992\n",
      "(38) Loss : 9.045831680297852\n",
      "(39) Loss : 8.97283935546875\n",
      "(40) Loss : 8.980220794677734\n",
      "(41) Loss : 9.03533935546875\n",
      "(42) Loss : 9.05711841583252\n",
      "(43) Loss : 9.007145881652832\n",
      "(44) Loss : 9.053740501403809\n",
      "(45) Loss : 8.990143775939941\n",
      "(46) Loss : 9.035475730895996\n",
      "(47) Loss : 8.99276065826416\n",
      "(48) Loss : 8.943490982055664\n",
      "*  it was spoopy doopy\n",
      "-   ac nah ac fils fils fils777 fils Wut\\nThey essenti app happen\\nThey jo Wut Wut happen\n",
      "\n",
      "*  Naw. Moose is on pubic transat and apparently embarrasingly canadian\\nnovoicechat\n",
      "-   aren aren This? This Warfortunately ques War lood happen,\\nIT p,\\nPatriots,, Sensual lood\n",
      "\n",
      "*  meese\\nheart\n",
      "-  all its:C fils nah fucking77pping two Lmaooo serious7 fucking Wut77 happen quesnconfused\n",
      "\n",
      "*  Oh shit... i have to cook for myself! Double ewe tea eff..\n",
      "-   bub teleports there letting letting Mand letting lood\\nPatriots letting Mand lood lood letting\\nPatriots\\nPatriots\\nPatriots liqu\\nPatriots bring\n",
      "\n",
      "*  lmao\\nmoose you gotta adult\\nMOOSE CAN I COOK FOR YOU?\n",
      "-   noes\\nTheyauramnconfused7\\nPatriots fils Sm\\nThey fils serious,, happen, fils, lood run,\n",
      "\n",
      "*  make some vegan onions\n",
      "-  7 Manty tele7,,,thol,,,, acpping,,,,7,\n",
      "\n",
      "*  Yes doc. You can cook for me.\n",
      "-   Orville77 its7 This its\\nPatriots,\\nPatriots\\nPatriots aren\\nPatriots lood\\nPatriots aren\\nPatriots lood lood aren\n",
      "\n",
      "*  yay!\n",
      "-   apparently\\nPatriots,\\nPatriotsject7ject Lmaooo ac7 serious launch, air,,, started NOT,\n",
      "\n",
      "*  But what if my onio s arent gloodan free?\\nNo adulting.\\nInfact forget food. It wil ruin my buzz.\n",
      "-   LmaooonTrain acallnTrain Thisallpping\\nTheyall Manty Thisall happen when_smiling happen loodauram Hes\n",
      "\n",
      "*  not if i add alcohol and roofies to the food\n",
      "-   Lmaooo Lmaooo emails strawberries\\nThey fils emails botpping happen fils happen\\nPatriots heardpping\\nPatriots Costanza fils apppping\n",
      "\n",
      "*  Do you mind if i\n",
      "-   plat apparentlyoll\\nPatriots\\nPatriots plat ac happen,\\nPatriots heard aren aren office7\\nPatriots office\\nPatriots\\nit7\n",
      "\n",
      "*  ...\\nPOLICE\n",
      "-  ish lesbianhie\\nIT7,\\nIT PM Look Look dec PM\\nIT dec\\nIT,\\nIT\\nITish\\nIT\n",
      "\n",
      "*  nice beer belly\n",
      "-   teleports apparently fucking fucking PM teleports teleports aren fucking fucking fucking dec decOW fucking convinc teleports aren teleports aren\n",
      "\n",
      "*  i spent all day binge watching The End of the Fing World and now that its over im sad\n",
      "-  nTHIS started\\nThey ac masc,ilsils masc masc filsmyan fils, ques fils Cof fils silver\\nPatriots\n",
      "\n",
      "*  the creator wants a second season\n",
      "-  } there\\nPatriots fils Cof\\nPatriots\\nPatriots,7,\\nPatriots\\nPatriots serious\\nPatriots,,\\nPatriots\\nPatriots\\nPatriots\\nPatriots\n",
      "\n",
      "*  me too\n",
      "-   live\\nThey fucking Lmaooo fuckingph fucking Lmaooo fucking fucking two fucking7 dec7 emails7\\nPatriots life fucking\n",
      "\n",
      "(49) Loss : 8.997105598449707\n",
      "(50) Loss : 9.02446174621582\n",
      "(51) Loss : 8.916228294372559\n",
      "(52) Loss : 8.926502227783203\n",
      "(53) Loss : 8.975886344909668\n",
      "(54) Loss : 8.973954200744629\n",
      "(55) Loss : 8.912267684936523\n",
      "(56) Loss : 8.905076026916504\n",
      "(57) Loss : 9.042473793029785\n",
      "(58) Loss : 9.06256103515625\n",
      "(59) Loss : 9.050753593444824\n",
      "(60) Loss : 8.878369331359863\n",
      "(61) Loss : 8.942240715026855\n",
      "(62) Loss : 8.940293312072754\n",
      "(63) Loss : 8.983154296875\n",
      "(64) Loss : 8.83463191986084\n",
      "*  there's some serious stuff going on in VC, ya'll need to join\n",
      "-   sod twonTrain movie Hes see I7 ac ac fils I tele happen\\nPatriots,,,\\nPatriots canadian\n",
      "\n",
      "*  You guys hit the gym?\n",
      "-  7777 heard I,,, run run7,,,, happen xkcd fils run\n",
      "\n",
      "*  Always\\nAlways\n",
      "-   plat fucking,,,7,, emails7,, fucking7 fucking, fucking77,\n",
      "\n",
      "*  Not enough\n",
      "-   aren7 aren\\nPatriots aren aren\\nPatriots stuffing aren aren aren aren aren aren aren aren run aren aren aren\n",
      "\n",
      "*  yep\n",
      "-  7 Cured,7 far\\nITish teleports,,,77 Look7 run stuffing, D\\nIT\n",
      "\n",
      "*  What keeps you motivated to work out?\n",
      "-  ferent ac its its happen\\nPatriots happen, happen happen teleportsils\\nPatriots Sensual happen happen, Costanza lesbian happen\n",
      "\n",
      "*  I want to work out but I literally hate sweating\n",
      "-   superbi I heard aren its happen I ac7,,,,,7 happen\\nPatriots7, happen\n",
      "\n",
      "*  Because I like to look good\\nAnd feel good\n",
      "-  7 I I7 aren aren I aren aren lood aren aren aren aren aren lood aren aren lesbian aren\n",
      "\n",
      "*  Motivation for me is it helps me with my anxiety\n",
      "-  \\nit aren\\nit aren seriousauram Lmaooo teleports Home77\\nPatriots happen Sensual Sensual\\nPatriots I Sensual Sensual Sensual\n",
      "\n",
      "*  This man is the only reason I still hit the gym, dude takes ten secs only to inspire me\\n\n",
      "-   fortune ac\\nThey two its when emails\\nPatriots run\\nPatriots ques its\\nPatriots Hes\\nPatriots\\nPatriots Cof happen\\nPatriots\\nPatriots\n",
      "\n",
      "*  and not as I hate working out, I just hate sweating\n",
      "-   botteez Lmaooo two fils Same Lmaoooall serious spe serious its,,7 run\\nPatriots app,,,\n",
      "\n",
      "*  Nothing better than turning on aggressive rap music and jogging my aggression away\n",
      "-  , emails,7, spe tele Manty serious\\nPatriots7 MantySyzz, Sensual,\\nPatriots,, Sensual\n",
      "\n",
      "*  Oh yeah sweating sucks ass\n",
      "-  ND face its face\\nPatriots happennconfused happen whis aren happen jonconfused happen happennconfusednconfused happen\\nPatriots happen\n",
      "\n",
      "*  CAN'T BE TOUCHED\n",
      "-  ph ac i run,,,,,,,7 movie,7,77,7\n",
      "\n",
      "*  Can't be moved\n",
      "-  77 Cof,, happen77 happen777 heard7 fucking7\\nPatriots RUMB77\n",
      "\n",
      "*  I always like make my room cool\\nso I don't sweat as much\n",
      "-   least ac7777 serious serious I aren\\nPatriots7n stuffing\\nPatriots happenND twonconfused\\nPatriots\n",
      "\n",
      "(65) Loss : 8.926176071166992\n",
      "(66) Loss : 8.892035484313965\n",
      "(67) Loss : 9.026247024536133\n",
      "(68) Loss : 8.80998706817627\n",
      "(69) Loss : 8.782666206359863\n",
      "(70) Loss : 8.738039016723633\n",
      "(71) Loss : 8.875407218933105\n",
      "(72) Loss : 8.935135841369629\n",
      "(73) Loss : 8.859807968139648\n",
      "(74) Loss : 9.044379234313965\n",
      "(75) Loss : 9.132220268249512\n",
      "(76) Loss : 8.940581321716309\n",
      "(77) Loss : 8.955682754516602\n",
      "(78) Loss : 8.828437805175781\n",
      "(79) Loss : 8.962200164794922\n",
      "(80) Loss : 9.024429321289062\n",
      "*  oh god\\nI can just imagine\\nlike a fleshlight that has a spot to put your phone on the side and plug in\\nfuckin talkin to it makes it vibrate\\ndirty talk siri\n",
      "-   serve girls happen its when aren\\nno happen aren happen I lood, aren aren Costanza aren I startednconfused\n",
      "\n",
      "*  you cant spell assistant without A S S\\ni can only get so erect @Rhoades\n",
      "-  auram aren live aren aren 7 see7 its its\\nPatriotsnconfused its\\nPatriots This\\nPatriots teleports\\nPatriots happen\n",
      "\n",
      "*  quitter talk\\nyou can get overerect if you really try\n",
      "-   need aren aren aren see MORE romantic arennconfused aren\\nPatriots aren aren, aren aren aren aren aren aren\n",
      "\n",
      "*  lewd\n",
      "-  ,ish fucking,,,,ish,,,,,, fucking,\\nIT,,,\n",
      "\n",
      "*  L I M I T B R E A K\n",
      "-   aren superbi least least\\nit I serious I I aren\\nit aren aren aren I I\\nPatriots I I I\n",
      "\n",
      "*  n e w d\n",
      "-   least aren I,777,77nIdk77,,,\\nit,,,\n",
      "\n",
      "*  shrewd\n",
      "-   fucking,,,, fucking fucking,thol,\\nIT fucking7,,,\\nIT,,thol\n",
      "\n",
      "*  d e w d\n",
      "-  \\nLmao apparently\\nPatriots,,,\\nit7 fucking7, apparently,,7, I7,7\n",
      "\n",
      "*  Bewb\n",
      "-   fucking fucking,, heard,,7,,,, run,,\\nIT,,,,\n",
      "\n",
      "*  Maximum overerect\n",
      "-   romanticnTr,,, you fucking\\nPatriots, aren,,nTr fuckingthol run,,,thol\n",
      "\n",
      "*  BOOBS\n",
      "-   Cured,,,,,,,,7\\nIT, Lmaooo,, far fucking,,,\n",
      "\n",
      "*  yes\\ntits knockers double d's\n",
      "-   botteez songs movie Cof live fucking\\nPatriots its run launch fucking happen\\nPatriots aren,\\nPatriots\\nPatriots\\nPatriots there\\nThey\n",
      "\n",
      "*  boops\n",
      "-  ,, far,,,,,, fucking,, D aren,,ish,\\nThey,\n",
      "\n",
      "*  stuck_out_tongue_closed_eyes\n",
      "-   Hes Orville jo\\nThey\\nThey johie aren\\nTheypping\\nThey stitches\\nchanting\\nTheyn\\nPatriots jo two BOO\\nThey\n",
      "\n",
      "*  milk bags\n",
      "-   teleports Lmaooo run Lmaooo77 Lmaooo7 fucking fucking D fucking7 teleports7 aren77 stuffing fucking\n",
      "\n",
      "*  whorebags lookers chest puppies\\nmilk jugs big breasts\\nshoulder boulders on her chest\n",
      "-   Dph fils7 I happen,, I emails, Orville,, happen, happen when,,\n",
      "\n",
      "(81) Loss : 8.899944305419922\n",
      "(82) Loss : 9.0267972946167\n",
      "(83) Loss : 8.870634078979492\n",
      "(84) Loss : 8.991043090820312\n",
      "(85) Loss : 8.920780181884766\n",
      "(86) Loss : 8.97394847869873\n",
      "(87) Loss : 9.02406120300293\n",
      "(88) Loss : 8.923385620117188\n",
      "(89) Loss : 8.898743629455566\n",
      "(90) Loss : 8.921545028686523\n",
      "(91) Loss : 8.913281440734863\n",
      "(92) Loss : 8.865653038024902\n",
      "(93) Loss : 8.783233642578125\n",
      "(94) Loss : 8.933977127075195\n",
      "(95) Loss : 8.920948028564453\n",
      "(96) Loss : 8.765165328979492\n",
      "*  But you're forgetting about Birdemic 3 addie\n",
      "-   apparently see see its its heard I I,, I, I, aren heard I, I aren\n",
      "\n",
      "*  Aka the best fucking film in the last 5 years or so\n",
      "-   happen nah Mand its aren heard} heard there its whenn even happen,,, I happen spicy\n",
      "\n",
      "*  THIS\\nT H I S\\nits so good\n",
      "-   attending teleports, you7 I I i,\\nPatriots there, need, need,,,,,\n",
      "\n",
      "*  dang it's not in english\n",
      "-  \\njapan its its serious two serious, Y apparently\\nPatriots7,7777,,\\nPatriots fucking\n",
      "\n",
      "*  Korean\\nSame guy who made oldboy\n",
      "-   romantic\\nPatriots\\nPatriots Cof fucking I I, I, aren I\\nPatriots, aren\\nPatriots,\\nPatriots aren I\n",
      "\n",
      "*  Korean/ Japanese\\nI cannot describe to you the levels of emotion that movie made me feel\n",
      "-   live its heard its its its its its happen heard heard its its aren I I, appnnconfused\n",
      "\n",
      "*  Guys how do I tell my parents to let me go out with a girl?\n",
      "-   heard its I I7 I heard heard I, its its happen, when, happen, happen,\n",
      "\n",
      "*  reee\n",
      "-  ,,,,7,,,,,,,,,, fucking,\\nIT,,\n",
      "\n",
      "*  it was so beautiful certain scenes were like paintings\n",
      "-  7 Costanzaph fils regular77 I77,,,,,,,,7,\n",
      "\n",
      "*  wow late BOT\n",
      "-   after happen7nconfused, fucking launch fucking fucking fucking7, fucking fucking fucking its, I, its\n",
      "\n",
      "*  thinking\n",
      "-  ,,, fucking\\nIT,7,,\\nIT,,,,,, fucking,,,\n",
      "\n",
      "*  yeah exactly\\nTho its tied for my best movie\n",
      "-   live thought happen thought I its thought aren happen aren happen7 aren aren happen happen  happen aren spicy\n",
      "\n",
      "*  with?\n",
      "-  nTrish, far, embarrasingly,,ish far,,\\nIT\\nIT,, far7 fleshlight\\nIT\n",
      "\n",
      "*  it on the same level as Ida for me\\nIda\n",
      "-   its its happen its I happen its its I its happen_smiling happen happen happen do happen\\nPatriotsn happen\n",
      "\n",
      "*  uhh where can i watch this movie?\n",
      "-   Jim Yess7 I i This need, need\\nPatriots\\nPatriots need lesbian lesbian\\nPatriots aren\\nPatriotsmyan Manty\\nPatriots\n",
      "\n",
      "*  haven't watched that one\n",
      "-   bother as heard7,,,n,77,, heard7,,7 happen heard\n",
      "\n",
      "(97) Loss : 9.046880722045898\n",
      "(98) Loss : 8.848209381103516\n",
      "(99) Loss : 8.943055152893066\n",
      "TESTING... \n",
      "\n",
      "\n",
      "\u00007, live I I7 heard7 I7\n",
      "\u0000, Iish its stress its as7 plat7\n",
      "\u0000\\nThey all its its ac, its,7 stress\n",
      "\u00007 thought? I, need its I see I\n",
      "\u0000, aren aren7 live I7 I I even\n",
      "\u00007 live heard7 live thought romantic\\nno live stress\n",
      "\u00007 its thoughtnconfused\\nit need heard7 its happen\n",
      "\u0000, aren meets I I7\\ndurr its stress\\ndurr\n",
      "\u0000,nconfused its its aren I77 I I\n",
      "\u0000, its7 aren77 its7 I7\n"
     ]
    }
   ],
   "source": [
    "reset = 1\n",
    "\n",
    "chats = chats\n",
    "\n",
    "tokenizer = load('tokenizer')\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "seq_len = 20\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "\n",
    "padd_seq = lambda ids: torch.tensor( (ids + [0]*(seq_len-len(ids)))[:seq_len] )\n",
    "get_right_mask = lambda ids : torch.stack([padding_mask(seq_len, len(tokens)) for tokens in ids]).unsqueeze(-2).repeat(1, seq_len, 1).unsqueeze(-3)\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size, src_len=seq_len, tgt_len=seq_len,\n",
    "    embed_dim=embed_dim, num_heads=num_heads,\n",
    "    num_layers=1, dropout=0.1,\n",
    ") if reset else load('model')\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n",
    "total_batches = 100 # 500\n",
    "batch_size = 16\n",
    "outputs = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "model.train()\n",
    "for i in range(total_batches):\n",
    "    chat = [''.join(chat.split(':')[1:]) for chat in chats[i*batch_size:i*batch_size+batch_size]]\n",
    "    # chat = [\"Hello world! how are you!\"]*batch_size\n",
    "    ids = [flattern(i) for i in tokenizer.encode(chat)]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    xpadded = torch.stack([padd_seq(tokens[:-1]) for tokens in ids])\n",
    "    right_mask = get_right_mask(ids)\n",
    "    output = model(tgt=xpadded, tgt_mask=right_mask)\n",
    "    \n",
    "    ypadded = torch.stack([padd_seq(tokens[1:]) for tokens in ids])\n",
    "    loss = criterion(output.view(-1, vocab_size), ypadded.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    predicted = torch.argmax(output, dim=-1)\n",
    "    decoded = tokenizer.decode([tuple(pred) for pred in predicted.tolist()])\n",
    "    \n",
    "    if i % 1 == 0: print(f'({i}) Loss : {loss.item()}');\n",
    "    # outputs.append(output); if i % 5 == 0 and i: outputs = torch.hstack(outputs); display_img(outputs); outputs=[];\n",
    "    # display_img(output);\n",
    "    if i % batch_size == 0 and 1: [(print('*', chat[i]), print('-', chat[i][0] + decoded[i]), print()) for i in range(len(decoded))]\n",
    "    # print()\n",
    "\n",
    "\n",
    "model = save(model, 'model')\n",
    "\n",
    "print(\"TESTING...\", \"\\n\"*2)\n",
    "\n",
    "for i in range(10):\n",
    "    text_start = [\"hi \"]\n",
    "    ids = [flattern(i) for i in tokenizer.encode(text_start)]\n",
    "    xpadded = torch.stack([padd_seq(tokens[:-1]) for tokens in ids])\n",
    "    output = model.generate(num_tokens=10, tgt=xpadded, tgt_mask=get_right_mask(ids))\n",
    "    decoded = tokenizer.decode((*output.tolist(),))\n",
    "    print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
